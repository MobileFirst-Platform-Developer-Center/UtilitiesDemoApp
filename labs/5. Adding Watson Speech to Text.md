# Adding Watson Speech To Text

1. Introduction
In this guide, we will add Watson Speech To Text service to our MobileFoundation application. IBM's Watson is the leading cognitive computing platform and provides a vast number of easy to use API's to access the artificial intelligence engine, allowing anybody to get started quickly and for free. There are a number of ways to integrate Watson services in your project, such as using a variety of Watson Developer Cloud SDKs or the individual Watson service REST APIs. All of these methods have their own corresponding documentation and labs available from multiple sources. There are also a number of cognitive services that Watson provides; you can browse all of them at the Bluemix catelog.

1. Why use Watson speech-to-text?
We chose to use the Watson speech-to-text service in this demo application to help the user quickly fill out a form. In this example, the application user records a voice message, which is then sent off to Watson, transcribed to text, and added to the form. The data could easily be mined for specific keywords that correspond to specific form fields.

You may be wondering why we are using Speech-to-Text when all of the major mobile operating systems have a built in voice functionality. We chose to use Watson instead of the built in speech-to-text for two reasons:

1 - When using the Android/iOS speech-to-text, the voice data is transcribed by Google or Apple services, respectively. This is sometimes not acceptable if your business deals with sensitive information that should not be stored on a third party cloud. Even if the data is transcribed by the device and not by a cloud service, it is still not accessible to you, the app developer - you cannot delete the data and guarantee that it cannot be accessed by others.

2 - With Watson, we have the ability to do further processing on the audio data. You will see in this example that we take the transcribed data from Watson, scan for key words (form field names), and fill out the form appropriately. With the built in speech functionality, you do not have the ability to manipulate the transcribed data before it gets sent to your application.

Finally, this guide will show you how to send a large data file using MobileFoundation Platform. This could be altered a bit to send an image file and use Watson Visual Recognition services, or any other Cognitive services provided by Watson.

1. Architecture
There are many different possible architectures that we could have used to integrate Watson services. We chose to record an audio file with the device and send that audio file to our Mobile Foundation server. There we save the audio file on the server and then send the file to Watson using the Watson Developer Cloud SDK. This gives us a chance to manipulate the audio file on our server if we need to - for example, you might need to further process the data or convert it to a different audio format that is accepted by Watson.

Another benefit of using the adapter functionality of Mobile Foundation was that it comes with built in security that is easy to configure. This is *essential* when using Watson services so that people cannot use your Watson service (that you pay for!) without your knowledge. It is true that Watson provides a credential mechanism that you can use to make a secure connection to the service, but using an adapter provides additional benefits, like using analytics to see how often your Watson endpoint is hit.

The major issue that we had while implementing this type of architecture was that the Mobile Foundation SDKs are not meant to be used with large chunks of data, like an audio file. In the documentation for JavaScript adapters, it says that the data you pass into the adapter is stored in memory, so passing in a large audio file may cause errors. *Note:* We did not run into any limitations with the size of the file when testing. Another problem that we had was that it we wanted to save the audio file on our server, and using the file system with Bluemix is not trivial because of the internal architecture. *Because of this*, this demo currently only works on a locally hosted Mobile Foundtion server.

2. Setting up Watson STT service on Bluemix
The first thing we want to do is set up the Watson Speech to Text service on Bluemix.

Navigate to your Bluemix catalog, search for Watson Speech to Text, and pick the Free version. Provision it to the same space that you are building this application in. You will be able to retrieve the username and password from your VCAP variables or from the "Credentials" tab when viewing the service on Bluemix dashboard. (Note, Watson provides the ability to request an API Access Token that you can use to make requests to the service without having to hard-code your username/password in the request. This is currently beyond the scope of this demo but is not difficult to implement.)

Once you have provisioned the Speech To Text service, you can test it out by sending audio files to the service using cURL. There are plenty of examples of this on the Speech to Text documentation. An example cURL request looks like this:

curl -X POST -u "username":"password" --header "Content-Type: audio/wav" --data-binary @audio.wav "https://stream.watsonplatform.net/speech-to-text/api/v1/recognize?max_alternatives=3&word_confidence=true"

That's it, we have successfully provisioned and tested an instance of the Watson Speech to Text service.

3. Updating the Utilities application
We will now update our Ionic application to allow it to record an audio file. The comments in the updated `report-equipment.js` file will walk you through the necessary changes to create a file on the device, initialize the audio recorder, and then upload the audio file to our node server. We also have to make some changes to `reportEquipment.html` file, such as adding the recorder buttons. Finally, because we are using some additional third party Cordova plugins, we will need to run a few `ionic` CLI commands to install these plugins.

Perhaps the most difficult part of using Watson to transcribe an audio file is actually sending the audio file through MobileFoundation adapters. We need to convert the audio file into a *base64* data string that we can then send to the service as a form parameters. Look carefully at the documentation and StackOverflow questions for Mobile Foundation adapters if you get stuck.

4. Testing and Troubleshooting
If you are unable to use the microphone to record audio and send it to Watson, there are a few different places that you need to check. You should first enable the debugger on the device and verify that you are successfully recording audio files. You can use a file browser application to verify that the audio file is being created and saved on the device correctly. You should be able to use a media application to play back this recorded file, since it is saved in the `PERSISTENT` Cordova file location, it should be accessible by other applications. You can also search for the `bufferCallback` function and uncomment it to verify that audio is actually being recorded (on Android only.)

Once you know that the audio file is being created and recorded successfully, you should make sure there are no errors in the device debugger when it comes to uploading the actual file. Make sure that the URL that you are uploading to correctly points to your node passthrough server, and make sure that you can actually make network connections (use the browser to navigate to the passthrough server.)

After that, if you are still having problems, you can run `cf logs {nodeApName} --recent` to view the logs from your passthrough server and verify that it is receiving the audio file and sending it to Watson. Verify your Watson Speech to Text username/password. You can navigate to the passthrough server on a browser and use the simple HTML form to upload an audio file from your browser. Also, take a look at the `keywords_threshold` value in the node app. You may need to change this value depending on how "clearly" the audio sample was recorded.

6. Next Steps
There are a variety of "next steps" that one can take to extend this lab to make it more useful and production ready. We leave these as exercises for the reader or for potential updates to this lab.
  0. Extend for iOS
  1. Extend for Bluemix
  4. Augmenting the utilities application with additional Watson services, such as Visual Recognition
  5. Handle multiple audio files
  6. Handle audio files only when connected to Wifi
  6. Parse the Watson transcript for keyword data

7. Lessons Learned
Over the course of developing this lab, we learned quite a bit about mobile development with Ionic, using Watson services, and using Mobile Foundation. These lessons are included in our Labs section on Github, but some major lessons are:
- Hybrid app development is hard, even with major improvements over the years to tooling and SDKs. Make sure you test in devices and simulators/emulators.
- Watson SDKs are always changing and the APIs can sometimes be a moving target. We had to pivot on this demo because the Watson service we originally wanted to show, text extraction from images, was temporarily removed from the catelog.
