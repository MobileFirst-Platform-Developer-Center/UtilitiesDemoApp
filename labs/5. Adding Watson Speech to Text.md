# Adding Watson Speech To Text

## 1. Introduction

In this guide, we will add the Watson [Speech To Text](http://www.ibm.com/watson/developercloud/doc/speech-to-text/) service to our MobileFoundation application. IBM's Watson is the leading cognitive computing platform and provides a vast number of easy to use API's to access the artificial intelligence engine, allowing anybody to get started quickly and for free. There are a number of ways to integrate Watson services into your architecture, such as using a variety of Watson Developer Cloud SDKs or the individual Watson service's REST APIs. All of these methods have their own corresponding documentation and labs available from multiple sources. There are also a number of cognitive services that Watson provides - you can browse all of them at the [Bluemix catelog]
(https://console.ng.bluemix.net/catalog/?taxonomyNavigation=watson&category=watson).

## 2. Why use Watson Speech-to-Text?

We chose to use the Watson speech-to-text service in this demo application to help the user quickly fill out a form. In this example, the application user records a voice message, which is then sent off to Watson, transcribed to text, and added to the form. The data could then easily be mined for specific keywords that correspond to specific form fields.

You may be wondering why we are using Speech-to-Text when all of the major mobile operating systems have a built in voice functionality. We chose to use Watson instead of the built in speech-to-text for two reasons:

1. When using the Android/iOS speech-to-text, the voice data is transcribed by Google or Apple services, respectively. This is sometimes not acceptable if your business deals with sensitive information that should not be stored on a third party cloud. Even if the data is transcribed by the device and not by a cloud service, it is still not accessible to you, the app developer - you cannot delete the data and guarantee that it cannot be accessed by others.

2. With Watson, we have the ability to do further processing on the audio data. You will see in this example that we take the transcribed data from Watson, scan for key words (form field names), and fill out the form appropriately. With the built in speech functionality, you do not have the ability to manipulate the transcribed data before it gets sent to your application. Another example of manipulating the data first would be to transcribe an audio file from one audio format to another before sending to Watson.

Finally, this guide will show you how to send a large data file using MobileFoundation Platform and the adapter technology. This could be altered a bit to send an image file and use Watson Visual Recognition services, or any other Cognitive services provided by Watson.

## 1. Architecture
There are many different possible architectures that we could have used to integrate Watson services. We chose to record an audio file with the device and send that audio file to our Mobile Foundation server. There we save the audio file on the server and then send the file to Watson using the Watson Developer Cloud SDK. This gives us a chance to manipulate the audio file on our server if we need to - for example, you might need to further process the data or convert it to a different audio format that is accepted by Watson.

Another benefit of using the adapter functionality of Mobile Foundation was that it comes with built in security that is easy to configure. This is *essential* when using Watson services so that people cannot use your Watson service (that you pay for!) without your knowledge. It is true that Watson provides a credential mechanism that you can use to make a secure connection to the service, but using an adapter provides additional benefits, like using analytics to see how often your Watson endpoint is hit.

## 2. Setting up Watson STT service on Bluemix
The first thing we want to do is set up the Watson Speech to Text service on Bluemix.

Navigate to your *Bluemix catalog*, search for *Watson Speech to Text*, and pick the *Standard* version. Provision it to the same space that you are building this application in. You will be able to retrieve the username and password from your VCAP variables or from the "Credentials" tab when viewing the service on Bluemix dashboard. (Note, Watson provides the ability to request an [API Access Token](http://www.ibm.com/watson/developercloud/doc/getting_started/gs-credentials.shtml) that you can use to make requests to the service without having to hard-code your username/password in the request. This is currently beyond the scope of this demo but is not difficult to implement.)

Once you have provisioned the Speech To Text service, you can test it out by sending audio files to the service using cURL. There are plenty of examples of this on the Speech to Text documentation. An example cURL request looks like this:

```curl -X POST -u "username":"password" --header "Content-Type: audio/wav" --data-binary @audio.wav "https://stream.watsonplatform.net/speech-to-text/api/v1/recognize?max_alternatives=3&word_confidence=true"```

That's it, we have successfully provisioned and tested an instance of the Watson Speech to Text service.

### 3. Building the Watson STT Adapter
We need to build a new [MobileFirst Adapter](https://mobilefirstplatform.ibmcloud.com/tutorials/en/foundation/8.0/adapters/) to receive an audio file and then send it to our Watson service. Our adapter is simple and will just write a temporary audio file to the system and then use the [Watson Developer SDK for Java](www.ibm.com/watson/developercloud/speech-to-text/api/v1/?java#recognize_sessionless_nonmp12) to recognize the audio in this file. This means that we will just have to wait while the system transcribes the data - if you read through the API, you will see that there are *session-based* architectures available that will provide more benefits, like "live" audio transcription. 

We will be building a [Java HTTP Adapter](https://mobilefirstplatform.ibmcloud.com/tutorials/en/foundation/8.0/adapters/java-adapters/java-http-adapter/) for this because it turns out to be the easiest for dealing with audio files, especially if we wanted to add in further processing to the audio file on the server side. Watson does provide a Node SDK that could be used in a JavaScript adapter, if you so choose. *Bonus:* Our adapter also includes a [custom request filter and interceptor](https://jersey.java.net/documentation/latest/filters-and-interceptors.html) to log the HEADERS of all requests coming into our adapter. This is extremely useful when developing and debugging adapters. 

If you have checked out the project from the Github repo, you can simply navigate to the */adapters/WatsonJava/* folder and run `mfpdev adapter build && mfpdev adapter deploy`, but it is more fun to create it from scratch. Navigate to your *adapters/* folder and type `mfpdev adapter create` - follow the prompts to create a Java adapter:
 prompt.png

### Add WatsonSDK to your Adapter's pom.xml
Open up the `pom.xml` file for your adapter. Here you can declare other dependencies from Maven that will be used in your Java project. Add the following dependencies, which will allow us to use Watson and some other libraries used for the request filter.
```
<dependency>
  			<groupId>com.ibm.watson.developer_cloud</groupId>
  			<artifactId>java-wrapper</artifactId>
  			<version>RELEASE</version>
  		</dependency>
  		<dependency>
  			<groupId>commons-io</groupId>
  			<artifactId>commons-io</artifactId>
  			<version>2.4</version>
  		</dependency>
  		<dependency>
  			<groupId>com.ibm.watson.developer_cloud</groupId>
  			<artifactId>java-sdk</artifactId>
  			<version>2.9.0</version>
  		</dependency>
  		  <dependency>
          <groupId>javax.json</groupId>
          <artifactId>javax.json-api</artifactId>
          <version>1.0</version>
      </dependency>
    <dependency>
        <groupId>javax.xml.bind</groupId>
        <artifactId>jaxb-api</artifactId>
        <version>2.2.12</version>
    </dependency>
  <dependency>
        <groupId>org.glassfish</groupId>
        <artifactId>javax.json</artifactId>
        <version>1.0.4</version>
      </dependency>
    <dependency>
    <groupId>org.glassfish.jersey.media</groupId>
    <artifactId>jersey-media-multipart</artifactId>
    <version>2.17</version>
</dependency>
      <dependency>
          <groupId>com.squareup.okhttp3</groupId>
          <artifactId>okhttp</artifactId>
          <version>3.4.1</version>
      </dependency>
      ```

#### Add the adapter code
Open up your *WatsonJavaResource.java* file and add the following code to create our endpoint:
 ```
 @Logged
  @POST
  @OAuthSecurity(enabled = false)
  @Path("/uploadBase64Wav")
  public Response handleUpload(@FormParam("audioFile") String base64wav, @QueryParam("keywords") String keywords, @Context HttpHeaders headers) throws Exception {

      // Convert the base64 string back into a wav file
      // http://stackoverflow.com/questions/23979842/convert-base64-string-to-image
      String base64 = base64wav.split(",")[1]; // remove the "data:audio/x-wav;base64" header
      byte[] wavBytes = javax.xml.bind.DatatypeConverter.parseBase64Binary(base64);
      System.out.println("bytes:" + wavBytes);
      System.out.println("Keywords:" + keywords);
      return callWatson(wavBytes, keywords);
  }
  ```
Look at the method signature. We will be accepting the audio file as a *base64 encoded string* as a FormParamater named *audioFile*. We are also accepting a query param named *keywords* which will be used to send specific keywords that Watson can identify - these would be the form paramaters that we want to automatically fill in. Also, note the custom `@Logged` annotation which will invoke our special receiver and print out all the meta data of any request that comes in. *Important:* we need to remove the header string that appears in the audio file so that Watson will recognize it. Otherwise, we might get errors from Watson like "unable to transcode data stream audio/wav -> audio/x-float-array".


**Note:** There is currently a bug in the product where you are unable to deploy large adapters through the command line.


### 3. Updating the Utilities application
We will now update our Ionic application to allow it to record an audio file. The comments in the `report-equipment.js` file will walk you through the necessary changes to create a file on the device, initialize the audio recorder, and then upload the audio file to our MFP adapter. We also have to make some changes to `reportEquipment.html` file, such as adding the recorder buttons. Finally, because we are using some additional third party Cordova plugins, we will need to run a few `ionic` CLI commands to install these plugins.

Perhaps the most difficult part of using Watson to transcribe an audio file is actually sending the audio file through MobileFoundation adapters. We need to convert the audio file into a *base64* data string that we can then send to the service as a form parameters. Look carefully at the documentation and StackOverflow questions for Mobile Foundation adapters if you get stuck.

4. Testing and Troubleshooting
If you are unable to use the microphone to record audio and send it to Watson, there are a few different places that you need to check. You should first enable the debugger on the device and verify that you are successfully recording audio files. You can use a file browser application to verify that the audio file is being created and saved on the device correctly. You should be able to use a media application to play back this recorded file, since it is saved in the `PERSISTENT` Cordova file location, it should be accessible by other applications. You can also search for the `bufferCallback` function and uncomment it to verify that audio is actually being recorded (on Android only.)

Once you know that the audio file is being created and recorded successfully, you should make sure there are no errors in the device debugger when it comes to uploading the actual file. Make sure that the URL that you are uploading to correctly points to your node passthrough server, and make sure that you can actually make network connections (use the browser to navigate to the passthrough server.)

After that, if you are still having problems, you can run `cf logs {nodeApName} --recent` to view the logs from your passthrough server and verify that it is receiving the audio file and sending it to Watson. Verify your Watson Speech to Text username/password. You can navigate to the passthrough server on a browser and use the simple HTML form to upload an audio file from your browser. Also, take a look at the `keywords_threshold` value in the node app. You may need to change this value depending on how "clearly" the audio sample was recorded.

6. Next Steps
There are a variety of "next steps" that one can take to extend this lab to make it more useful and production ready. We leave these as exercises for the reader or for potential updates to this lab.
  0. Extend for iOS
  1. Extend for Bluemix
  4. Augmenting the utilities application with additional Watson services, such as Visual Recognition
  5. Handle multiple audio files
  6. Handle audio files only when connected to Wifi
  6. Parse the Watson transcript for keyword data

7. Lessons Learned
Over the course of developing this lab, we learned quite a bit about mobile development with Ionic, using Watson services, and using Mobile Foundation. These lessons are included in our Labs section on Github, but some major lessons are:
- Hybrid app development is hard, even with major improvements over the years to tooling and SDKs. Make sure you test in devices and simulators/emulators.
- Watson SDKs are always changing and the APIs can sometimes be a moving target. We had to pivot on this demo because the Watson service we originally wanted to show, text extraction from images, was temporarily removed from the catelog.
